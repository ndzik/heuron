{-# LANGUAGE InstanceSigs #-}
{-# LANGUAGE RoleAnnotations #-}
{-# LANGUAGE ScopedTypeVariables #-}
{-# OPTIONS_GHC -Wno-deferred-out-of-scope-variables #-}

-- | Any activation function can be implemented outside of this library by
-- defining a data type that implements the `ActivationFunction` and
-- `Differentiable` typeclasses.
module Heuron.V1.Batched.Activation where

import Control.Lens
import GHC.TypeLits
import Heuron.Functions hiding (Datum (..))
import Linear (Metric (dot), scaled, transpose, (!-!))
import Linear.Matrix (identity, (!*), (!*!), (*!), (*!!))
import Linear.V
import Linear.V1
import Linear.Vector (Additive (liftI2), (*^))
import Prelude hiding (zip)

class Differentiable a where
  derivative ::
    (KnownNat b, KnownNat n, Dim b, Dim n) =>
    -- | Datatype implementing the Differentiable instance.
    a ->
    -- | Derivation function:
    -- original batch of already weighted inputs
    -- -> output generated by a using the original batch of inputs
    -- -> derivative result
    ( V b (V n Double) ->
      V b (V n Double) ->
      V b (V n Double)
    )

class (Differentiable a) => ActivationFunction a where
  activation :: (Dim b, Dim n) => a -> (V b (V n Double) -> V b (V n Double))

data ReLU = ReLU

instance Differentiable ReLU where
  derivative ReLU oi _ = fmap (fmap drelu) oi
    where
      drelu x
        | x > 0 = 1
        | otherwise = 0

instance ActivationFunction ReLU where
  activation ReLU = fmap (fmap relu)
    where
      relu = max 0

-- | Softmax activation function carrying the input dimension `d` as a phantom
-- type.
data Softmax d = Softmax

type role Softmax nominal

instance Differentiable (Softmax d) where
  -- ∂Softmax_j/∂x_k = S_j * δ_jk - S_j * S_k
  derivative ::
    (Dim n, Dim b) =>
    Softmax d ->
    V b (V n Double) ->
    V b (V n Double) ->
    V b (V n Double)
  derivative Softmax _ os = dS <$> os
    where
      dS :: (Dim n) => V n Double -> V n Double
      dS sample = fmap sum $ sXδ !-! sjXsk
        where
          mSample = V1 sample
          tmSample = transpose mSample
          sjXsk = tmSample !*! mSample
          sXδ = scaled sample

instance ActivationFunction (Softmax d) where
  activation Softmax inputs =
    let maxInput = maximum $ maximum <$> inputs
        expInputs = ((\v -> exp (v - maxInput)) <$>) <$> inputs
        sumExpInputs = sum <$> expInputs
        normalizeRow ei v = (/ ei) <$> v
        normalizedActivation = liftI2 normalizeRow sumExpInputs expInputs
     in normalizedActivation
