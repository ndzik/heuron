-- | Any activation function can be implemented outside of this library by
-- defining a data type that implements the `ActivationFunction` and
-- `Differentiable` typeclasses.
module Heuron.V1.Batched.Activation where

import Control.Lens
import GHC.TypeLits
import Heuron.Functions hiding (Datum (..))
import Linear (transpose, (!-!))
import Linear.Matrix (identity, (!*), (!*!), (*!), (*!!))
import Linear.V
import Linear.Vector (Additive (liftI2))
import Prelude hiding (zip)

class Differentiable a where
  derivative ::
    (Dim b, Dim n) =>
    -- | Datatype implementing the Differentiable instance.
    a ->
    -- | Derivation function:
    -- original batch of already weighted inputs
    -- -> output generated by a using the original batch of inputs
    -- -> derivative result
    ( V b (V n Double) ->
      V b (V n Double) ->
      V b (V n Double)
    )

class (Differentiable a) => ActivationFunction a where
  activation :: (Dim b, Dim n) => a -> (V b (V n Double) -> V b (V n Double))

data ReLU = ReLU

instance Differentiable ReLU where
  derivative ReLU oi _ = fmap (fmap drelu) oi
    where
      drelu x
        | x > 0 = 1
        | otherwise = 0

instance ActivationFunction ReLU where
  activation ReLU = fmap (fmap relu)
    where
      relu = max 0

-- | Softmax activation function carrying the input dimension `d` as a phantom
-- type.
data Softmax (d :: Nat) = Softmax

-- TODO: Implement softmax Differentiable.
instance Differentiable (Softmax d) where
  derivative Softmax oi os = undefined
    where
      singularOutput = os ^?! ix 0
      xxx = undefined
      dSoftmax v = undefined

instance ActivationFunction (Softmax d) where
  activation Softmax inputs =
    let expInputs = (exp <$>) <$> inputs
        sumExpInputs = sum <$> expInputs
        normalizeRow ei v = (/ ei) <$> v
        normalizedActivation = liftI2 normalizeRow sumExpInputs expInputs
     in normalizedActivation
