{-# LANGUAGE DeriveGeneric #-}
{-# LANGUAGE InstanceSigs #-}
{-# LANGUAGE RoleAnnotations #-}
{-# LANGUAGE ScopedTypeVariables #-}
{-# OPTIONS_GHC -Wno-unrecognised-pragmas #-}

{-# HLINT ignore "Eta reduce" #-}

-- | Any activation function can be implemented outside of this library by
-- defining a data type that implements the `ActivationFunction` and
-- `Differentiable` typeclasses.
module Heuron.V1.Batched.Activation.Function where

import Codec.Serialise
import Control.Lens
import Data.Data (Proxy (..))
import qualified Data.Vector as Vec
import GHC.Generics
import GHC.TypeLits
import Heuron.Functions hiding (Datum (..))
import Linear (Metric (dot), scaled, transpose, (!-!))
import Linear.Matrix (identity, (!*), (!*!), (*!), (*!!))
import Linear.V
import Linear.V1
import Linear.Vector (Additive (liftI2), (*^))
import Prelude hiding (zip)

class Differentiable a where
  -- | The return value of the derivative function. Some activation functions
  -- return a scalar, others return a vector, and others return a matrix.
  -- This type family allows the user to specify the return type of the
  -- derivative function.
  type DerivativeReturn a b

  -- | The default return value of the derivative function. Works for any
  -- activation function operating on scalar values.
  type DerivativeReturn a b = b

  derivative ::
    (KnownNat b, KnownNat n, Dim b, Dim n) =>
    -- | Datatype implementing the Differentiable instance.
    a ->
    -- | Derivation function:
    -- original batch of already weighted inputs
    -- -> output generated by using the original batch of inputs
    -- -> derivative result
    ( V b (V n Double) ->
      V b (V n Double) ->
      DerivativeReturn a (V b (V n Double))
    )

class (Differentiable a) => BackpropableFunction a where
  backpropagate ::
    (Dim b, Dim n) =>
    a ->
    -- The return value of the derivative function for `a`.
    DerivativeReturn a (V b (V n Double)) ->
    -- The gradients of a previous layer for a given batch of inputs.
    V b (V n Double) ->
    V b (V n Double)

class (BackpropableFunction a) => ActivationFunction a where
  activation :: (KnownNat b, KnownNat n, Dim b, Dim n) => a -> (V b (V n Double) -> V b (V n Double))

data ReLU = ReLU deriving (Generic, Show, Eq)

instance Serialise ReLU

instance Differentiable ReLU where
  derivative ReLU oi _ = fmap (fmap drelu) oi
    where
      drelu x
        | x > 0 = 1
        | otherwise = 0

instance BackpropableFunction ReLU where
  backpropagate ReLU = mergeEntriesWith (*)

instance ActivationFunction ReLU where
  activation ReLU = fmap (fmap relu)
    where
      relu = max 0

-- | Softmax activation function.
data Softmax = Softmax deriving (Generic, Show, Eq)

instance Serialise Softmax

instance Differentiable Softmax where
  type DerivativeReturn Softmax (V b (V n Double)) = V b (V n (V n Double))

  -- ∂Softmax_j/∂x_k = S_j * δ_jk - S_j * S_k
  derivative ::
    (Dim n, Dim b) =>
    Softmax ->
    V b (V n Double) ->
    V b (V n Double) ->
    DerivativeReturn Softmax (V b (V n Double))
  derivative Softmax _ os = dS <$> os
    where
      dS :: (Dim n) => V n Double -> V n (V n Double)
      dS sample = sXδ !-! sjXsk
        where
          mSample = V1 sample
          tmSample = transpose mSample
          sjXsk = tmSample !*! mSample
          sXδ = scaled sample

instance BackpropableFunction Softmax where
  backpropagate ::
    forall b n.
    (Dim b, Dim n) =>
    Softmax ->
    DerivativeReturn Softmax (V b (V n Double)) ->
    V b (V n Double) ->
    V b (V n Double)
  backpropagate Softmax dActivation prevGradients = liftI2 applyJacobi dActivation prevGradients
    where
      applyJacobi :: V n (V n Double) -> V n Double -> V n Double
      applyJacobi jm prevGradient = jm <&> (`dot` prevGradient)

instance ActivationFunction Softmax where
  activation :: forall b n. (KnownNat b) => Softmax -> V b (V n Double) -> V b (V n Double)
  activation Softmax inputs =
    let batchSize = fromIntegral $ natVal (Proxy @b)
        maxInputs = maximum <$> inputs
        indexedInputs = Vec.indexed (toVector inputs)
        expInputs =
          fromVector' @b $
            ( \(idx, vs) ->
                fmap
                  ( \v ->
                      let maxInput = maxInputs ^?! ix idx
                       in exp (v - maxInput)
                  )
                  vs
            )
              <$> indexedInputs
        sumExpInputs = sum <$> expInputs
        normalizeRow ei vs = (\v -> if v == 0 then 0 else v / ei) <$> vs
        normalizedActivation = liftI2 normalizeRow sumExpInputs expInputs
     in normalizedActivation
    where
      fromVector' :: forall n a. (KnownNat n) => Vec.Vector a -> V n a
      fromVector' v = case fromVector @n v of
        Just v' -> v'
        Nothing -> error "fromVector' failed"
